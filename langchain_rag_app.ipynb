{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1e2c745-0700-45ee-be85-d043c2fdf401",
   "metadata": {},
   "source": [
    "# Building RAG Applications with Langchain\n",
    "\n",
    "This notebook demonstrates how to build a Retrieval Augmented Generation (RAG) application using Langchain. We will cover two main use cases: generating quizzes from web content and extracting information from runbooks.\n",
    "\n",
    "## Setup\n",
    "\n",
    "First, we need to set up our environment by loading API keys and importing necessary libraries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "ecd7f628-add2-4303-bc0b-cc32cfa84be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import bs4\n",
    "import getpass\n",
    "import requests\n",
    "from langchain import hub\n",
    "from bs4 import BeautifulSoup\n",
    "from dotenv import load_dotenv\n",
    "from langchain_core.documents import Document\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain.document_loaders import PyPDFLoader, TextLoader\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "        \n",
    "if not os.environ.get(\"ANTHROPIC_API_KEY\"):\n",
    "  os.environ[\"ANTHROPIC_API_KEY\"] = getpass.getpass(\"Enter API key for Anthropic: \")\n",
    "\n",
    "from langchain.chat_models import init_chat_model\n",
    "\n",
    "llm = init_chat_model(\"claude-3-5-sonnet-latest\", model_provider=\"anthropic\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2983fcf4-066a-4d9a-a9c8-9caa08de644d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.environ.get(\"OPENAI_API_KEY\"):\n",
    "  os.environ[\"OPENAI_API_KEY\"] = getpass.getpass(\"Enter API key for OpenAI: \")\n",
    "\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b0e6d4a-3a03-4572-a8b6-9755e078a76b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_chroma import Chroma\n",
    "\n",
    "vector_store = Chroma(embedding_function=embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9df2a85-4f30-4dbd-90eb-9301dc03b4b3",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- We load the necessary API keys for Anthropic and OpenAI.\n",
    "- We initialize the language model (LLM) and embeddings model.\n",
    "- We set up Chroma as our vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f118bfc2-7719-42b0-827b-45d5d1b0d547",
   "metadata": {},
   "source": [
    "## Usecase 1: Generating Quizzes from Web Content\n",
    "\n",
    "In this use case, we will fetch content from specified webpages, create a vector store, and generate a quiz based on the content.\n",
    "\n",
    "### Fetching and Cleaning Web Content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "737547d2-83b4-458f-a734-63c35e82673c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_clean_web_content(url):\n",
    "    \"\"\"Fetches and extracts only the main content from a webpage.\"\"\"\n",
    "    try:\n",
    "        response = requests.get(url, timeout=10)\n",
    "        response.raise_for_status()\n",
    "        \n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Remove unwanted elements\n",
    "        for tag in [\"script\", \"style\", \"nav\", \"footer\", \"header\", \"aside\", \"form\", \"noscript\"]:\n",
    "            for element in soup.find_all(tag):\n",
    "                element.decompose()\n",
    "        \n",
    "        # Extract meaningful sections (modify selectors based on site structure)\n",
    "        main_content = soup.find(\"article\") or soup.find(\"main\") or soup.find(\"div\", class_=\"content\") or soup.find(\"body\")\n",
    "        if not main_content:\n",
    "            return None\n",
    "\n",
    "        text = main_content.get_text(separator=\"\\n\", strip=True)\n",
    "\n",
    "        # Ensure we get a minimum amount of text (to avoid indexing useless content)\n",
    "        if len(text) < 300:  \n",
    "            print(f\"Skipped {url} due to insufficient content.\")\n",
    "            return None\n",
    "\n",
    "        return Document(page_content=text, metadata={\"source\": url})\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"Failed to load {url}: {e}\")\n",
    "        return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "17de7d92-2d1c-47b6-943c-3efbf04954c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_webpages = [\n",
    "    \"https://www.datastax.com/guides/hierarchical-navigable-small-worlds\",\n",
    "    \"https://medium.com/@datastax/how-does-hierarchical-navigable-small-world-hnsw-power-genai-ee0ee24f8fce\"\n",
    "]\n",
    "\n",
    "# Load and clean content from webpages\n",
    "clean_docs = [fetch_clean_web_content(url) for url in list_of_webpages]\n",
    "clean_docs = [doc for doc in clean_docs if doc]  # Remove None values\n",
    "\n",
    "# Now, use clean_docs for vector indexing\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = splitter.split_documents(clean_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "badb08be-3682-4348-9f71-dbd5f61aa8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = vector_store.add_documents(all_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd967c58-282d-4831-81f5-8a7a19bce253",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- We define a list of URLs to fetch content from.\n",
    "- The fetch_clean_web_content function fetches the content, removes unwanted HTML elements, and extracts the main text.\n",
    "- We split the documents into smaller chunks for better vector search.\n",
    "- We add the documents to our vector store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "89454a69-1947-43fc-a9d9-fb9ebb440b96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Debug\n",
    "# retrieved_docs = vector_store.similarity_search(\"Generate me quiz questions on Nonissuers Communication?\", k=5)\n",
    "# for doc in retrieved_docs:\n",
    "#     print(doc.page_content[:500])  # Print first 500 characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59725b9f-5d91-4327-88c5-ec0b5a305641",
   "metadata": {},
   "source": [
    "## Generating Quiz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "d0ed5a2c-1500-4602-a702-feef0ce892fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    quiz_questions: str\n",
    "    structured_summary: str\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vector_store.similarity_search(state[\"question\"], k=5)\n",
    "    \n",
    "    # Combine document content\n",
    "    combined_content = \"\\n\\n\".join(doc.page_content for doc in retrieved_docs)\n",
    "    \n",
    "    # Validate content\n",
    "    if not combined_content.strip() or len(combined_content) < 300:\n",
    "        return {\"context\": None, \"error\": \"Retrieved context is too fragmented or insufficient.\"}\n",
    "\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def summarize_context(state: State):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    \n",
    "    summary_prompt = \"\"\"\n",
    "    Given the following text, extract the key concepts in a structured manner:\n",
    "    \n",
    "    Text:\n",
    "    {context}\n",
    "\n",
    "    Output the response as a structured summary.\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(summary_prompt.format(context=docs_content))\n",
    "    return {\"structured_summary\": response.content}\n",
    "\n",
    "def generate_quiz(state: State):\n",
    "    structured_summary = state[\"structured_summary\"]\n",
    "    \n",
    "    quiz_prompt = \"\"\"\n",
    "    Based on the structured summary below, generate 5 multiple-choice quiz questions. \n",
    "    Each question should have 4 options with one correct answer.\n",
    "\n",
    "    Structured Summary:\n",
    "    {summary}\n",
    "\n",
    "    Output Format:\n",
    "    Q1: [Question here]\n",
    "    A) Option 1\n",
    "    B) Option 2\n",
    "    C) Option 3\n",
    "    D) Option 4\n",
    "    Correct Answer: [Correct Option]\n",
    "    \"\"\"\n",
    "    \n",
    "    response = llm.invoke(quiz_prompt.format(summary=structured_summary))\n",
    "    return {\"quiz_questions\": response.content}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "728ced3a-f887-4a6f-8246-0106c10c2cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "graph_builder = StateGraph(State).add_sequence([retrieve, summarize_context, generate_quiz])\n",
    "\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "f73f04f7-9655-42c1-ac6c-90c80f17018f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here are 5 multiple-choice questions based on the structured summary:\n",
      "\n",
      "Q1: What is a key architectural component of HNSW?\n",
      "A) Single-layer structure\n",
      "B) Tiered architecture with multiple layers\n",
      "C) Random node distribution\n",
      "D) Fixed-path routing\n",
      "Correct Answer: B\n",
      "\n",
      "Q2: How has HNSW evolved from its predecessor NSW?\n",
      "A) From logarithmic to linear complexity\n",
      "B) From linear to exponential complexity\n",
      "C) From polylogarithmic to logarithmic complexity\n",
      "D) From exponential to linear complexity\n",
      "Correct Answer: C\n",
      "\n",
      "Q3: Which feature is NOT one of the main applications of HNSW?\n",
      "A) Vector databases\n",
      "B) Blockchain processing\n",
      "C) Similarity search operations\n",
      "D) AI and data science applications\n",
      "Correct Answer: B\n",
      "\n",
      "Q4: What is one of the primary implementation considerations for HNSW?\n",
      "A) Network bandwidth requirements\n",
      "B) CPU clock speed limitations\n",
      "C) Practical limitations on memory usage\n",
      "D) Graphics processing capabilities\n",
      "Correct Answer: C\n",
      "\n",
      "Q5: Which functionality is central to HNSW's design?\n",
      "A) Data encryption\n",
      "B) File compression\n",
      "C) Data backup\n",
      "D) Optimized search paths\n",
      "Correct Answer: D\n"
     ]
    }
   ],
   "source": [
    "response = graph.invoke({\"question\": \"Generate a quiz on HNSW indexing\"})\n",
    "\n",
    "print(response[\"quiz_questions\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f7c127-a362-4520-bf47-2b021493b595",
   "metadata": {},
   "source": [
    "### Explanation:\n",
    "\n",
    "- We define a prompt for question-answering.\n",
    "- We create a state graph to manage the flow of our application.\n",
    "- The retrieve function fetches relevant documents from the vector store.\n",
    "- The summarize_context function generates a structured summary of the retrieved content.\n",
    "- The generate_quiz function creates a quiz based on the summary.\n",
    "- We invoke the graph with a question and print the generated quiz."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c0fd732-1d64-49af-87b7-108c2d3c2e58",
   "metadata": {},
   "source": [
    "## Usecase 2: Extracting Information from Runbooks\n",
    "\n",
    "In this use case, we will extract information from PDF and text files in a specified directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "6fd571ac-f9d8-4490-80c4-b8083c815d02",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents_from_directory(directory: str):\n",
    "    \"\"\"Loads all PDFs and text files from a given directory.\"\"\"\n",
    "    documents = []\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        if filename.endswith(\".pdf\"):\n",
    "            print(f\"Loading PDF: {filename}\")\n",
    "            loader = PyPDFLoader(file_path)\n",
    "            docs = loader.load()\n",
    "        \n",
    "        elif filename.endswith(\".txt\"):\n",
    "            print(f\"Loading Text File: {filename}\")\n",
    "            loader = TextLoader(file_path)\n",
    "            docs = loader.load()\n",
    "        \n",
    "        else:\n",
    "            print(f\"Skipping unsupported file: {filename}\")\n",
    "            continue\n",
    "        \n",
    "        # Attach metadata\n",
    "        for doc in docs:\n",
    "            doc.metadata[\"source\"] = filename  # Track document source\n",
    "        \n",
    "        documents.extend(docs)\n",
    "    \n",
    "    return documents\n",
    "\n",
    "docs = load_documents_from_directory(DATA_DIR)\n",
    "chunks = chunk_documents(docs)\n",
    "runbook_vector_store = Chroma(embedding_function=embeddings)\n",
    "index_documents(runbook_vector_store, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad3cda67-543d-44a8-a402-14b872421558",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- The load_documents_from_directory function loads PDF and text files from the directory.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272bbb02-fb6d-4b62-a0fb-9cd2779c3d22",
   "metadata": {},
   "source": [
    "#### Helper Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "7c1c5d1c-bdd5-4375-921c-cbd963384b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_documents(docs, chunk_size=500, chunk_overlap=200):\n",
    "    \"\"\"Splits documents into smaller chunks for better vector search.\"\"\"\n",
    "    splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    return splitter.split_documents(docs)\n",
    "\n",
    "def create_vector_store():\n",
    "    \"\"\"Initializes Chroma vector store.\"\"\"\n",
    "    return Chroma(embedding_function=embeddings)\n",
    "\n",
    "\n",
    "def index_documents(runbook_vector_store, docs):\n",
    "    \"\"\"Adds chunked documents to the vector store.\"\"\"\n",
    "    runbook_vector_store.add_documents(docs)\n",
    "\n",
    "def retrieve_similar_docs(runbook_vector_store, query, k=5):\n",
    "    \"\"\"Retrieves top-k similar documents for a given query.\"\"\"\n",
    "    return runbook_vector_store.similarity_search(query, k=k)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163ebafc-6e86-40f3-917d-bbc84df85924",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- `chunk_documents`: Splits the documents into smaller chunks using RecursiveCharacterTextSplitter.\n",
    "- `create_vector_store`: Initializes a Chroma vector store with the specified embeddings.\n",
    "- `index_documents`: Adds the document chunks to the vector store."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccf8920-a2ce-4050-bf14-cf2974cc0a18",
   "metadata": {},
   "source": [
    "### Defining Runbook Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "10a9cb44-2387-470d-9337-3d6626b935b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define prompt for question-answering\n",
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "\n",
    "class RunbookState(TypedDict):\n",
    "    question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "def rb_retrieve(state: RunbookState):\n",
    "    retrieved_docs = runbook_vector_store.similarity_search(state[\"question\"], k=5)\n",
    "    return {\"context\": retrieved_docs}\n",
    "\n",
    "def rb_generate(state: RunbookState):\n",
    "    docs_content = \"\\n\\n\".join(doc.page_content for doc in state[\"context\"])\n",
    "    prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "    generate_prompt = \"\"\"\n",
    "        You are an expert runbook assistant. Given the user's question and the relevant information from the runbooks, provide a clear and actionable response.\n",
    "        \n",
    "        User Question: {question}\n",
    "                \n",
    "        Ensure your response is accurate, practical, and easy to understand.\n",
    "        \n",
    "        Output your response in the following structured format:\n",
    "        \n",
    "        **Core Problem:** [Clearly state the problem]\n",
    "        \n",
    "        **Action Plan:**\n",
    "        1.  [Action 1]\n",
    "        2.  [Action 2]\n",
    "        3.  [Action 3]\n",
    "        \n",
    "        **Key Information:**\n",
    "        [Summarize relevant details from the runbook information]\n",
    "        \n",
    "        Remember, your goal is to provide fast, practical, and accurate assistance to the user.\n",
    "        \"\"\"\n",
    "    messages = prompt.invoke({\"question\": generate_prompt.format(question=state['question']), \"context\": docs_content})\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "runbook_graph_builder = StateGraph(RunbookState).add_sequence([rb_retrieve, rb_generate])\n",
    "runbook_graph_builder.add_edge(START, \"rb_retrieve\")\n",
    "runbook_graph = runbook_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7711184-9b08-4718-a39f-fb6f3ba32afd",
   "metadata": {},
   "source": [
    "#### Explanation:\n",
    "\n",
    "- `RunbookState`: Defines the state for our runbook application.\n",
    "- `rb_retrieve`: Retrieves relevant documents from the runbook vector store.\n",
    "- `rb_generate`: Generates an answer based on the retrieved documents using the LLM.\n",
    "- We build and compile the state graph for the runbook application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "e21ca0b9-cd5f-4211-b513-a4026ade9bdd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading Text File: AtherStream-Runbook.txt\n",
      "Loading Text File: sample_runbook1.txt\n"
     ]
    }
   ],
   "source": [
    "# Define directory containing runbooks (PDFs & Text)\n",
    "DATA_DIR = \"data/runbooks\"\n",
    "\n",
    "# Load documents\n",
    "docs = load_documents_from_directory(DATA_DIR)\n",
    "\n",
    "# Chunk documents\n",
    "chunks = chunk_documents(docs)\n",
    "\n",
    "# Create & populate vector store\n",
    "# runbook_vector_store.delete_collection()\n",
    "runbook_vector_store = create_vector_store()\n",
    "index_documents(runbook_vector_store, chunks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a231aa0-b006-45bd-ab62-705258d813c1",
   "metadata": {},
   "source": [
    "### Querying the Runbook System"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "fae79032-c51c-4d58-83ac-335175b9045b",
   "metadata": {},
   "outputs": [],
   "source": [
    "runbook_graph_builder = StateGraph(RunbookState).add_sequence([rb_retrieve, rb_generate])\n",
    "\n",
    "runbook_graph_builder.add_edge(START, \"rb_retrieve\")\n",
    "runbook_graph = runbook_graph_builder.compile()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "98f7dab7-a34a-4911-923c-37f0169c10bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "[Runbook expert] How can I help you today? How to check for kinesis metrics for AetherStream?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Core Problem:** \n",
      "Need to monitor Kinesis metrics for AetherStream data processing pipeline\n",
      "\n",
      "**Action Plan:**\n",
      "1. Access AWS CloudWatch console as AetherStream uses CloudWatch for monitoring\n",
      "2. Navigate to Kinesis Data Streams metrics section\n",
      "3. Monitor metrics related to AetherStream's Kinesis components through CloudWatch dashboard\n",
      "\n",
      "**Key Information:**\n",
      "- AetherStream uses AWS Kinesis Data Streams for high-volume data ingestion and processing\n",
      "- System architecture flows from data sources through Kinesis to Lambda and then to downstream systems\n",
      "- Monitoring is implemented through CloudWatch integration\n"
     ]
    }
   ],
   "source": [
    "# Query the system\n",
    "\n",
    "query = input('[Runbook expert] How can I help you today?')\n",
    "\n",
    "runbook_response = runbook_graph.invoke({\"question\": query})\n",
    "print(runbook_response['answer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b586fc-6a26-4e02-a50c-1a633d960791",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38cf2600-8b11-4e54-9317-84e068dad264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
